# backend (.env file)

# URL to the LM Studio OpenAI-compatible endpoint (testing only)
# LM_API_URL=http://127.0.0.1:1234/v1/chat/completions

# --- existing RAG LLM (phi-4) ---
LLM_API_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=lm-studio
LLM_MAIN_MODEL=exaone-3.5-2.4b-instruct

# --- new Judge LLM (locally-hosted) ---
JUDGE_API_BASE_URL=http://localhost:1234/v1
JUDGE_API_KEY=lm-studio
JUDGE_MODEL_PATH=exaone-3.5-2.4b-instruct:2

# --- evaluation defaults ---
JUDGE_METRICS=["faithfulness"]
HALLUCI_THRESHOLD=0.75